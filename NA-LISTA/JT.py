# -*- coding: utf-8 -*-
"""JT_latest in use lastyear_LISTA_normal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w_5sx4HtHivKAiqU29B0EEjJRYWITBu-
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
import random
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import time
import os

def soft_thr(input_, theta_):  #soft thresholding function used for ISTA/LISTA steps
    return F.relu(input_-theta_)-F.relu(-input_-theta_)

class LISTA(nn.Module):
    def __init__(self, m, n, Dict, numIter, L, device):
        super(LISTA, self).__init__()
        self._W = nn.Linear(in_features=m, out_features=n, bias=False)
        self._S = nn.Linear(in_features=n, out_features=n, bias=False)
        self.thr = nn.Parameter(torch.ones(1, 1) * 0.1 / L, requires_grad=True)  # initialization directly
        self.numIter = numIter
        self.A = Dict
        self.L = L
        self.device = device

    def weights_init(self): # custom weight initialization called on the network
        A = self.A
        L = self.L

        # Initialize S and B matrices
        S = torch.from_numpy(np.eye(A.shape[1]) - (1/L) * np.matmul(A.T, A)).float().to(self.device)
        B = torch.from_numpy((1/L) * A.T).float().to(self.device)

        # Assign weights correctly without re-wrapping
        with torch.no_grad():  # Disable gradient tracking during initialization
            self._S.weight.copy_(S)
            self._W.weight.copy_(B)
            self.thr.copy_(torch.ones(1, 1).to(self.device) * 0.1 / L) # For learning single thr for every layer

    def forward(self, y):
        x = []
        d = torch.zeros(y.shape[0], self.A.shape[1], device = self.device)

        for iter in range(self.numIter):
            d = soft_thr(self._W(y) + self._S(d), self.thr)
            x.append(d)
        return x


# defining custom dataset
class dataset(Dataset):
    def __init__(self, X, Y):
        super().__init__()
        self.X = X
        self.Y = Y

    def __len__(self):
        return self.Y.shape[0]

    def __getitem__(self, idx):
        return self.X[idx, :], self.Y[idx, :]


def LISTA_train(X, Y, X_val, Y_val, D, numEpochs, numLayers, device, learning_rate, batch_size):

    m, n = D.shape
    Train_size = Y.shape[1]

    # convert the data into tensors
    Y_t = torch.from_numpy(Y.T)  # Y of the train dataset
    Y_t = Y_t.float().to(device)

    Y_val_t = torch.from_numpy(Y_val.T) # Y of the validation dataset
    Y_val_t = Y_val_t.float().to(device)

    D_t = torch.from_numpy(D.T)  # D is the measurement matrix(often referred to as A)  *** y = Ax + \eta ***
    D_t = D_t.float().to(device)

    X_t = torch.from_numpy(X.T) # X of the train dataset
    X_t = X_t.float().to(device)

    X_val_t = torch.from_numpy(X_val.T) # X of the validation dataset
    X_val_t = X_val_t.float().to(device)

    #train and val datasets and dataloader
    dataset_train = dataset(X_t, Y_t)
    dataset_valid = dataset(X_val_t, Y_val_t)
    print('DataSet size is: ', dataset_train.__len__())
    dataLoader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle = True)
    dataLoader_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle = False)

    # compute the max eigen value of the D'*D
    T = np.matmul(D.T, D)
    eg, _ = np.linalg.eig(T)
    eg = np.abs(eg)
    L = np.max(eg)*1.001  #value a little over the greatest eigenvalue of D'*D

    # initalize the network
    net = LISTA(m, n, D, numLayers, L = L, device = device)
    net = net.float().to(device)
    net.weights_init()  #weights intialization

    # build the optimizer and criterion
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate, betas = (0.9, 0.999))

    # list of losses at every epoch
    train_loss_list = []
    valid_loss_list = []


    best_model = net; best_loss = 1e6
    lr = learning_rate
    #**** Training phase **********
    print('Training >>>>>>>>>>>>>>')
    for epoch in range(numEpochs):

        T_tot_loss = 0
        total_samples = 0
        net.train()

        for iter, data in enumerate(dataLoader_train):
            X_GT_batch, Y_batch = data
            X_batch_hat = net(Y_batch.float())  # get the outputs
            loss = criterion(X_batch_hat[numLayers-1].float(), X_GT_batch.float())

            T_tot_loss += loss.item()*X_GT_batch.size(0)
            total_samples += X_GT_batch.size(0)

            optimizer.zero_grad()   #clear the gradients
            loss.backward()     # compute the gradiettns
            optimizer.step()    # Update the weights

        train_loss_list.append(T_tot_loss / total_samples)  


        # Validation stage
        with torch.no_grad():
            net.eval()
            V_tot_loss = 0
            val_tot_samples = 0
            for iter, data in enumerate(dataLoader_valid):
                X_GT_batch, Y_batch = data
                X_batch_hat = net(Y_batch.float())  # get the outputs
                loss = criterion(X_batch_hat[numLayers-1].float(), X_GT_batch.float())

                V_tot_loss += loss.item()*X_GT_batch.size(0)
                val_tot_samples += X_GT_batch.size(0)

            valid_loss_list.append(V_tot_loss / val_tot_samples)  

            if best_loss > V_tot_loss:
                best_model = net
                best_loss = V_tot_loss

        if epoch % 1 == 0 :
            print('Epoch: {:03d} :   Training Loss: {:<20.15f}  |  Validation Loss: {:<20.15f}'.format(epoch, T_tot_loss/ total_samples, V_tot_loss/ val_tot_samples))

    print('Training Completed')

    output_dir = "plots"  #plotting training and validation loss curves
    os.makedirs(output_dir, exist_ok=True)

    plt.figure()
    plt.plot(train_loss_list, label='Train Loss', color='blue')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Loss')
    plt.grid(True)
    plt.savefig(os.path.join(output_dir, f"train_loss_epoch.png"))
    plt.close()

    plt.figure()
    plt.plot(valid_loss_list, label='Validation Loss', color='orange')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Validation Loss')
    plt.grid(True)
    plt.savefig(os.path.join(output_dir, f"val_loss_epoch.png"))
    plt.close()

    return best_model

def LISTA_test(net, Y, D, device):

    # convert the data into tensors
    Y_t = torch.from_numpy(Y.T)
    if len(Y.shape) <= 1:
        Y_t = Y_t.view(1, -1)
    Y_t = Y_t.float().to(device)
    D_t = torch.from_numpy(D.T)
    D_t = D_t.float().to(device)

    with torch.no_grad():
        # Compute the output
        net.eval()
        X_lista = net(Y_t.float())
        if len(Y.shape) <= 1:
            X_lista = X_lista.view(-1)
        X_final = X_lista[-1].cpu().numpy()
        X_final = X_final.T

    return X_final



# Dataset class
class datagen():
    def __init__(self, n, m, k, dist):
        self.n = n #x vector dimension
        self.m = m #measurement dimension
        self.k = k #sparsity


   #genearting sparse X signals with a min distance between the non sparse points of 10
    def generate_sparse_signal(self, p):
        x_lst = np.zeros((self.n, p))
        for i in range(x_lst.shape[1]):
            indices=[]
            available_indices = list(range(self.n))

            for mf in range(self.k):
                available_indices = [idx for idx in available_indices if all(abs(idx - chosen_idx) >= 10 for chosen_idx in indices)]
                if not available_indices:
                    break
                chosen_index = np.random.choice(available_indices)
                indices.append(chosen_index)
                available_indices = [idx for idx in available_indices if abs(idx - chosen_index) >= 10]
            x = np.random.uniform(0.2, 1.0, self.k)
            x_lst[indices, i] = x
        return x_lst



    # Generate measurement matrix A
    def generate_measurement_matrix(self):
        mes_mat = np.random.randn(self.m, self.n)
        norms = np.linalg.norm(mes_mat, axis=0)
        mes_mat = mes_mat/norms
        return mes_mat

    # Generate measurements Y
    def generate_measurement(self, A, x_lst):
        return np.matmul(A, x_lst)

    # Add noise to measurements Y
    def add_noise(self, y_lst, sigma):
        for i in range(y_lst.shape[1]):
            noise = np.random.randn(y_lst.shape[0])*sigma
            y_lst[:,i] += noise
        return y_lst
    
    #genarting x, y pairs
    def data_gen(self, A, X, sigma_lst):
        Y = []
        for sigma in sigma_lst:
            y = self.generate_measurement(A, X)
            y_noisy = self.add_noise(y, sigma)
            Y.append(y_noisy)

        return X, Y

seed = 80
np.random.seed(seed)
random.seed(42)
torch.manual_seed(42)
torch.cuda.manual_seed_all(42)  # for multi-GPU
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

m = 30 #N_y
n = 100 #N_x
sparsity = 3 #K

dist = 10
numTrain = 43000 #N
loss_function = nn.MSELoss()

datagenerator = datagen(n, m, sparsity, dist)
A = datagenerator.generate_measurement_matrix() #dictionary matrix
X = datagenerator.generate_sparse_signal(numTrain)  #common X samples for all domains

""" This is just for checking"""
####################################
A_bf = np.load("A.npy")
X_bf = np.load("X.npy")

print(np.array_equal(A, A_bf), np.array_equal(X_bf, X))
####################################

def data(sigma):  #splitting data into train, test and val
    X1, Y1 = datagenerator.data_gen(A, X, [sigma])
    print(isinstance(X1, np.ndarray))
    SNR = 10*np.log10(np.mean(np.matmul(A, X1)**2, axis = 0)/sigma**2)
    print("SNR:", np.mean(SNR), "SNR std:", np.std(SNR))

    X1 = X1.T
    Y1 = Y1[0].T

    X1_train_val, X1_test, Y1_train_val, Y1_test = train_test_split(X1, Y1, test_size=0.2, random_state=42)
    X1_train, X1_val, Y1_train, Y1_val = train_test_split(X1_train_val, Y1_train_val, test_size=0.3, random_state=42)

    X1_train, X1_val, X1_test = X1_train.T, X1_val.T, X1_test.T
    Y1_train, Y1_val, Y1_test = Y1_train.T, Y1_val.T, Y1_test.T

    return X1_train, Y1_train, X1_val, Y1_val, X1_test, Y1_test

sigma = 0.1 #domain1
X1_train, Y1_train, X1_val, Y1_val, X1_test, Y1_test = data(sigma)

SNR = 10*np.log10(np.mean(np.matmul(A, X1_train)**2, axis = 0)/sigma**2)
print(np.mean(SNR), np.std(SNR))

sigma = 0.03  #domain2
X2_train, Y2_train, X2_val, Y2_val, X2_test, Y2_test = data(sigma)

SNR = 10*np.log10(np.mean(np.matmul(A, X2_train)**2, axis = 0)/sigma**2)
print(np.mean(SNR), np.std(SNR))

sigma = 0.005 #domain3
X3_train, Y3_train, X3_val, Y3_val, X3_test, Y3_test = data(sigma)

SNR = 10*np.log10(np.mean(np.matmul(A, X3_train)**2, axis = 0)/sigma**2)
print(np.mean(SNR), np.std(SNR))

def data_mix(seed, x1, y1, x2, y2, x3, y3):
    l = x1.shape[1]
    print("l : ", l)
    sel = l//1

    idx1 = seed.choice(l, sel, replace=False)
    idx2 = seed.choice(l, sel, replace=False)
    idx3 = seed.choice(l, sel, replace=False)


    x_mixed = np.concatenate((x1[:,idx1], x2[:,idx2], x3[:,idx3]), axis=1)
    y_mixed = np.concatenate((y1[:,idx1], y2[:,idx2], y3[:,idx3]), axis=1)

    indices = np.arange(x_mixed.shape[1])
    print("indices", indices)
    np.random.shuffle(indices)
    X_mixed = x_mixed[:, indices]
    Y_mixed = y_mixed[:, indices]

    return X_mixed, Y_mixed

seed_mixed = 10
np_seed_mixed = np.random.RandomState(seed_mixed)
X_train_mixed, Y_train_mixed = data_mix(np_seed_mixed, X1_train, Y1_train, X2_train, Y2_train, X3_train, Y3_train)
X_val_mixed, Y_val_mixed = data_mix(np_seed_mixed, X1_val, Y1_val, X2_val, Y2_val, X3_val, Y3_val)

print(torch.cuda.is_available())
print(torch.cuda.get_device_name(0))

if torch.cuda.is_available():
    device = 'cuda:0'
else:
    device = 'cpu'

print("device: ",device)

learning_rate = 8e-5
numEpochs = 50
numLayers = 15
batch_size = 32

start = time.time()
#training loop
net_mixed_1 = LISTA_train(X_train_mixed, Y_train_mixed, X_val_mixed, Y_val_mixed, A, 165, numLayers, device, learning_rate, batch_size)
print(f'time taken is {time.time() - start}')

"""Testing"""
g = 0.3
X_est_5 = LISTA_test(net_mixed_1, Y1_test, A, device)
l1=loss_function(torch.tensor(X_est_5),torch.tensor(X1_test))
l2=loss_function(torch.tensor(X1_test),torch.zeros_like(torch.tensor(X1_test)))
dbloss = l1.item()/l2.item()
mse_5 = np.log10(dbloss)*10

hr_list = np.array([np.sum((a != 0) & (np.abs(a - b) <= g*a)) / np.sum(a != 0) * 100 for a, b in zip(X1_test.T, X_est_5.T)])
hr_5 = np.mean(hr_list)


print(f"TEST1: MSE: {mse_5}, HR1: mean {hr_5}")


X_est_15 = LISTA_test(net_mixed_1, Y2_test, A, device)
l1=loss_function(torch.tensor(X_est_15),torch.tensor(X2_test))
l2=loss_function(torch.tensor(X2_test),torch.zeros_like(torch.tensor(X2_test)))
dbloss = l1.item()/l2.item()
mse_15 = np.log10(dbloss)*10

hr_list = np.array([np.sum((a != 0) & (np.abs(a - b) <= g*a)) / np.sum(a != 0) * 100 for a, b in zip(X2_test.T, X_est_15.T)])
hr_15 = np.mean(hr_list)


print(f"TEST2: MSE: {mse_15},  HR1: mean {hr_15}")


X_est_30 = LISTA_test(net_mixed_1, Y3_test, A, device)
l1=loss_function(torch.tensor(X_est_30),torch.tensor(X3_test))
l2=loss_function(torch.tensor(X3_test),torch.zeros_like(torch.tensor(X3_test)))
dbloss = l1.item()/l2.item()
mse_30 = np.log10(dbloss)*10

hr_list = np.array([np.sum((a != 0) & (np.abs(a - b) <= g*a)) / np.sum(a != 0) * 100 for a, b in zip(X3_test.T, X_est_30.T)])
hr_30 = np.mean(hr_list)

print(f"TEST3: MSE: mean {mse_30}, HR1: mean {hr_30}")


torch.save(net_mixed_1, "new_epoch_165_lr_8e_5.pth")


