# -*- coding: utf-8 -*-
"""DDTDA_6_mar_27 feb trying inconsistencies.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xfafopxOFW3F8_JKThxOn05BoangaOeK
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.utils.data import TensorDataset
from sklearn.model_selection import train_test_split
import math
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import time
import os
import random

def soft_thr(input_, theta_):
    return F.relu(input_-theta_)-F.relu(-input_-theta_)

class LISTA_visu(nn.Module):
    def __init__(self, m, n, Dict, numIter, L, device):
        super(LISTA_visu, self).__init__()
        self._W = nn.Linear(in_features = m, out_features = n, bias=False)
        self._S = nn.Linear(in_features = n, out_features = n, bias=False)
        self.m = m
        self.n = n

        self.numIter = numIter
        self.A = Dict
        self.L = L
        self.device = device

        self.fc1 = nn.Linear(m + Dict.shape[0] * Dict.shape[1], 512)
        self.bn1 = nn.BatchNorm1d(512)

        self.dropout = nn.Dropout(p=0.5)
        self.fc2 = nn.Linear(512, 256)
        self.bn2 = nn.BatchNorm1d(256)
        self.dropout2 = nn.Dropout(p=0.5)
        self.fc2_int1 = nn.Linear(256, 128)

        self.fc3 = nn.Linear(128,64)
        self.fc3_int1 = nn.Linear(64, 32)
        self.fc4 = nn.Linear(32, 1)   # Output one threshold value

    def weights_init(self):
        A = self.A
        L = self.L

        S = torch.from_numpy(np.eye(A.shape[1]) - (1/L)*np.matmul(A.T, A)).float().to(self.device)
        B = torch.from_numpy((1/L)*A.T).float().to(self.device)

        with torch.no_grad():  # Disable gradient tracking during initialization
            self._S.weight.copy_(S)
            self._W.weight.copy_(B)


        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')
        nn.init.zeros_(self.fc1.bias)

        nn.init.kaiming_normal_(self.fc2.weight, nonlinearity='relu')
        nn.init.zeros_(self.fc2.bias)

        nn.init.kaiming_normal_(self.fc2_int1.weight, nonlinearity='relu')
        nn.init.zeros_(self.fc2_int1.bias)

        nn.init.kaiming_normal_(self.fc3.weight, nonlinearity='relu')
        nn.init.zeros_(self.fc3.bias)

        nn.init.kaiming_normal_(self.fc3_int1.weight, nonlinearity='relu')
        nn.init.zeros_(self.fc3_int1.bias)

        nn.init.kaiming_normal_(self.fc4.weight, nonlinearity='relu')
        nn.init.zeros_(self.fc4.bias)


    def forward(self, y, sigma):
        t = []
        d = torch.zeros(y.shape[0], self.A.shape[1], device = self.device)

        self.smul = torch.from_numpy(self.A).to(self.device).view(1, -1)
        tensors_to_concat = [self.smul] * y.shape[0]
        repeated_tensor = torch.cat(tensors_to_concat, dim=0) # concat the tensors
        self.smul = repeated_tensor.float()
        x = torch.cat((y, self.smul), dim=1).float()  # Flatten A and concatenate
        x = torch.relu(self.bn1(self.fc1(x))).float()
        x = self.dropout(x).float()
        x = torch.relu(self.bn2(self.fc2(x))).float()
        x = self.dropout2(x).float()
        x = torch.relu(self.fc2_int1(x)).float()
        x = torch.relu(self.fc3(x)).float()
        x = torch.relu(self.fc3_int1(x)).float()
        c = torch.relu(self.fc4(x)).float()

        x = []
        d = torch.zeros(y.shape[0], self.A.shape[1], device = self.device)
        for iter in range(self.numIter):
            d = soft_thr(self._W(y) + self._S(d), c*0.1)
            x.append(d)
        return x


# defining custom dataset
class dataset(Dataset):
    def __init__(self, X, Y, sigma):
        super().__init__()
        self.X = X
        self.Y = Y
        self.sigma = sigma
    def __len__(self):
        return self.Y.shape[0]

    def __getitem__(self, idx):
        return self.X[idx, :], self.Y[idx, :], self.sigma[idx, :] # since the input to the datset is tensor


#%%
def LISTA_train(X,Y, X_val, Y_val, D, numEpochs, numLayers, device, learning_rate, batch_size, sigma, sigma_val):

    m, n = D.shape
    Train_size = Y.shape[1]

    # convert the data into tensors
    Y_t = torch.from_numpy(Y.T)
    Y_t = Y_t.float().to(device)

    Y_val_t = torch.from_numpy(Y_val.T)
    Y_val_t = Y_val_t.float().to(device)

    D_t = torch.from_numpy(D.T)
    D_t = D_t.float().to(device)

    X_t = torch.from_numpy(X.T)
    X_t = X_t.float().to(device)

    X_val_t = torch.from_numpy(X_val.T)
    X_val_t = X_val_t.float().to(device)

    sigma_t = torch.from_numpy(sigma.T)
    sigma_t = sigma_t.float().to(device)

    sigma_val_t = torch.from_numpy(sigma_val.T)
    sigma_val_t = sigma_val_t.float().to(device)

    # train and val datasets
    dataset_train = dataset(X_t, Y_t, sigma_t)
    dataset_valid = dataset(X_val_t, Y_val_t, sigma_val_t)
    print('DataSet size is: ', dataset_train.__len__())
    dataLoader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle = True, num_workers = 0)
    dataLoader_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle = False, num_workers = 0)

    # compute the max eigen value of the D'*D
    T = np.matmul(D.T, D)
    eg, _ = np.linalg.eig(T)
    eg = np.abs(eg)
    L = np.max(eg)*1.001

    #initializing the network
    net = LISTA_visu(m, n, D, numLayers, L, device)
    net = net.float().to(device)
    net.weights_init()

    # build the optimizer and criterion
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate, betas = (0.9, 0.999))

    # list of losses at every epoch
    train_loss_list = []
    valid_loss_list = []

    best_model = net; best_loss = 1e6
    lr = learning_rate
    # *****Training phase *******
    print('Training >>>>>>>>>>>>>>')
    for epoch in range(numEpochs):

        T_tot_loss = 0
        total_samples = 0
        net.train()

        for iter, data in enumerate(dataLoader_train):
            X_GT_batch, Y_batch, sigma_batch = data
            X_batch_hat = net(Y_batch.float(), sigma_batch.float())  # get the outputs, second input argument is the sigma appropiate to the SNR
            loss = criterion(X_batch_hat[numLayers-1].float(), X_GT_batch.float())

            T_tot_loss += loss.item()*X_GT_batch.size(0)
            total_samples += X_GT_batch.size(0)

            optimizer.zero_grad()   #clear the gradients
            loss.backward()     # compute the gradiettns
            optimizer.step()    # Update the weights

        train_loss_list.append(T_tot_loss / total_samples)


        # Validation stage
        with torch.no_grad():
            net.eval()
            V_tot_loss = 0
            val_tot_samples = 0
            for iter, data in enumerate(dataLoader_valid):

                X_GT_batch, Y_batch, sigma_batch = data
                X_batch_hat = net(Y_batch.float(), sigma_batch.float())  # get the outputs
                loss = criterion(X_batch_hat[numLayers-1].float(), X_GT_batch.float())

                V_tot_loss += loss.item()*X_GT_batch.size(0)
                val_tot_samples += X_GT_batch.size(0)

            valid_loss_list.append(V_tot_loss / val_tot_samples)

            if best_loss > V_tot_loss:
                best_model = net
                best_loss = V_tot_loss

        if epoch % 1 == 0 :
            print('Epoch: {:03d} : Training Loss: {:<20.15f}| Validation Loss: {:<20.15f}'.format( epoch, T_tot_loss / total_samples, V_tot_loss / val_tot_samples))

    print('Training Completed')

    output_dir = "plots"
    os.makedirs(output_dir, exist_ok=True)

    plt.figure()
    plt.plot(train_loss_list, label='Train Loss', color='blue')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Loss')
    plt.grid(True)
    plt.savefig(os.path.join(output_dir, f"train_loss_epoch.png"))
    plt.close()

    plt.figure()
    plt.plot(valid_loss_list, label='Validation Loss', color='orange')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Validation Loss')
    plt.grid(True)
    plt.savefig(os.path.join(output_dir, f"val_loss_epoch.png"))
    plt.close()

    return best_model

def LISTA_test(net, Y, D, device, sigma):

    # convert the data into tensors
    Y_t = torch.from_numpy(Y.T)
    if len(Y.shape) <= 1:
        Y_t = Y_t.view(1, -1)
    Y_t = Y_t.float().to(device)

    D_t = torch.from_numpy(D.T)
    D_t = D_t.float().to(device)

    sigma_t = torch.from_numpy(sigma.T)
    if len(sigma.shape) <= 1:
        sigma_t = sigma_t.view(1, -1)
    sigma_t = sigma_t.float().to(device)

    with torch.no_grad():
        net.eval()
        X_lista = net(Y_t.float(), sigma_t.float())
        if len(Y.shape) <= 1:
            X_lista = X_lista.view(-1)
        X_final = X_lista[-1].cpu().numpy()
        X_final = X_final.T

    return X_final

# Dataset class
class datagen():
    def __init__(self, n, m, k):
        self.n = n  #sparse vector dimension
        self.m = m  #measurement dimension
        self.k = k  #sparsity


    def generate_sparse_signal(self, p):
        x_lst = np.zeros((self.n, p))
        for i in range(x_lst.shape[1]):
            indices=[]
            available_indices = list(range(self.n))
            for mf in range(self.k):
                available_indices = [idx for idx in available_indices if all(abs(idx - chosen_idx) >= 10 for chosen_idx in indices)]
                if not available_indices:
                    break
                chosen_index = np.random.choice(available_indices)
                indices.append(chosen_index)
                available_indices = [idx for idx in available_indices if abs(idx - chosen_index) >= 10]
            x = np.random.uniform(0.2, 1.0, self.k)
            x_lst[indices, i] = x
        return x_lst


    # Generate measurement matrix A
    def generate_measurement_matrix(self):
        mes_mat = np.random.randn(self.m, self.n)
        norms = np.linalg.norm(mes_mat, axis=0)
        mes_mat = mes_mat/norms
        return mes_mat

    # Generate measurements Y
    def generate_measurement(self, A, x_lst):
        return np.matmul(A, x_lst)

    # Add noise to measurements Y
    def add_noise(self, y_lst, sigma):
        for i in range(y_lst.shape[1]):
            noise = np.random.randn(y_lst.shape[0])*sigma
            y_lst[:,i] += noise
        return y_lst

    def data_gen(self, A, X, sigma_lst):
       Y = []
       SIGMA = []
       for sigma in sigma_lst:
            y = self.generate_measurement(A, X)
            y_noisy = self.add_noise(y,sigma)
            Y.append(y_noisy)
            sigma_vector = np.empty([1, np.shape(y_noisy)[1]])
            sigma_vector.fill(sigma)
            SIGMA.append(sigma_vector)
       return X, np.array(Y), np.array(SIGMA)

seed = 80
np.random.seed(seed)
random.seed(42)
torch.manual_seed(42)
torch.cuda.manual_seed_all(42)  # for multi-GPU
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

m = 30 #N_y
n = 100 #N_x
sparsity = 3 #K

dist = 10
numTrain = 43000 #N
loss_function = nn.MSELoss()

datagenerator = datagen(n, m, sparsity)
A = datagenerator.generate_measurement_matrix() #generating dictionary matrix
X = datagenerator.generate_sparse_signal(numTrain) #generating sparse samples

def data(sigma):
    X1, Y1, sigma1 = datagenerator.data_gen(A, X, [sigma])
    SNR = 10*np.log10(np.mean(np.matmul(A, X1)**2, axis = 0)/sigma**2)
    print("SNR:", np.mean(SNR), "SNR std:", np.std(SNR))

    X1 = X1.T
    Y1 = Y1[0].T
    sigma1 = sigma1[0].T

    X1_train_val, X1_test, Y1_train_val, Y1_test, sigma1_train_val, sigma1_test = train_test_split(X1, Y1, sigma1, test_size=0.2, random_state=42)
    X1_train, X1_val, Y1_train, Y1_val, sigma1_train, sigma1_val = train_test_split(X1_train_val, Y1_train_val, sigma1_train_val, test_size=0.3, random_state=42)

    X1_train, X1_val, X1_test = X1_train.T, X1_val.T, X1_test.T
    Y1_train, Y1_val, Y1_test = Y1_train.T, Y1_val.T, Y1_test.T
    sigma1_train, sigma1_val, sigma1_test = sigma1_train.T, sigma1_val.T, sigma1_test.T

    return X1_train, Y1_train, sigma1_train, X1_val, Y1_val, sigma1_val, X1_test, Y1_test, sigma1_test

sigma = 0.1 #domain1
X1_train, Y1_train, sigma1_train, X1_val, Y1_val, sigma1_val, X1_test, Y1_test, sigma1_test = data(sigma)

SNR = 10*np.log10(np.mean(np.matmul(A, X1_train)**2, axis = 0)/sigma**2)
print(np.mean(SNR), np.std(SNR))

sigma = 0.03 #domain2
X2_train, Y2_train, sigma2_train, X2_val, Y2_val, sigma2_val, X2_test, Y2_test, sigma2_test = data(sigma)

SNR = 10*np.log10(np.mean(np.matmul(A, X2_train)**2, axis = 0)/sigma**2)
print(np.mean(SNR), np.std(SNR))

sigma = 0.005 #domain3
X3_train, Y3_train, sigma3_train, X3_val, Y3_val, sigma3_val, X3_test, Y3_test, sigma3_test = data(sigma)

SNR = 10*np.log10(np.mean(np.matmul(A, X3_train)**2, axis = 0)/sigma**2)
print(np.mean(SNR), np.std(SNR))

def data_mix(seed, x1, y1, sigma1, x2, y2, sigma2, x3, y3, sigma3):
    l = x1.shape[1]
    print("l : ", l)
    sel = l//1

    sigma1, sigma2, sigma3 = np.array(sigma1), np.array(sigma2), np.array(sigma3)

    idx1 = seed.choice(l, sel, replace=False)
    idx2 = seed.choice(l, sel, replace=False)
    idx3 = seed.choice(l, sel, replace=False)


    x_mixed = np.concatenate((x1[:,idx1], x2[:,idx2], x3[:,idx3]), axis=1)
    y_mixed = np.concatenate((y1[:,idx1], y2[:,idx2], y3[:,idx3]), axis=1)
    sigma_mixed = np.concatenate((sigma1[:, idx1], sigma2[:, idx2], sigma3[:, idx3]), axis = 1)

    indices = np.arange(x_mixed.shape[1])
    np.random.shuffle(indices)
    X_mixed = x_mixed[:, indices]
    Y_mixed = y_mixed[:, indices]
    Sigma_mixed = sigma_mixed[:, indices]


    return X_mixed, Y_mixed, Sigma_mixed

seed_mixed = 10
np_seed_mixed = np.random.RandomState(seed_mixed)
X_train_mixed, Y_train_mixed, sigma_train_mixed = data_mix(np_seed_mixed, X1_train, Y1_train, sigma1_train, X2_train, Y2_train, sigma2_train, X3_train, Y3_train, sigma3_train)
X_val_mixed, Y_val_mixed, sigma_val_mixed = data_mix(np_seed_mixed, X1_val, Y1_val, sigma1_val, X2_val, Y2_val, sigma2_val, X3_val, Y3_val, sigma3_val)

print(torch.cuda.is_available())
print(torch.cuda.get_device_name(0))

if torch.cuda.is_available():
    device = 'cuda:0'
else:
    device = 'cpu'

print("device: ",device)

learning_rate = 1e-4  #5e-4
numEpochs = 20
numLayers = 15
batch_size = 32

net_mixed = LISTA_train(X_train_mixed, Y_train_mixed, X_val_mixed, Y_val_mixed, A, 120, numLayers, device, learning_rate, batch_size, sigma_train_mixed, sigma_val_mixed)

g= 0.3
X_est_5 = LISTA_test(net_mixed, Y1_test, A, device, sigma1_test)
l1=loss_function(torch.tensor(X_est_5),torch.tensor(X1_test))
l2=loss_function(torch.tensor(X1_test),torch.zeros_like(torch.tensor(X1_test)))
dbloss = l1.item()/l2.item()
mse_5 = np.log10(dbloss)*10

hr_list = np.array([np.sum((a != 0) & (np.abs(a - b) <= g*a)) / np.sum(a != 0) * 100 for a, b in zip(X1_test.T, X_est_5.T)])
hr_5 = np.mean(hr_list)

print(f"TEST1: MSE: {mse_5}, HR1: mean {hr_5}")

X_est_15 = LISTA_test(net_mixed, Y2_test, A, device, sigma2_test)
l1=loss_function(torch.tensor(X_est_15),torch.tensor(X2_test))
l2=loss_function(torch.tensor(X2_test),torch.zeros_like(torch.tensor(X2_test)))
dbloss = l1.item()/l2.item()
mse_15 = np.log10(dbloss)*10

hr_list = np.array([np.sum((a != 0) & (np.abs(a - b) <= g*a)) / np.sum(a != 0) * 100 for a, b in zip(X2_test.T, X_est_15.T)])
hr_15 = np.mean(hr_list)

print(f"TEST2: MSE: {mse_15}, HR1: mean {hr_15}")

X_est_30 = LISTA_test(net_mixed, Y3_test, A, device, sigma3_test)
l1=loss_function(torch.tensor(X_est_30),torch.tensor(X3_test))
l2=loss_function(torch.tensor(X3_test),torch.zeros_like(torch.tensor(X3_test)))
dbloss = l1.item()/l2.item()
mse_30 = np.log10(dbloss)*10

hr_list = np.array([np.sum((a != 0) & (np.abs(a - b) <= g*a)) / np.sum(a != 0) * 100 for a, b in zip(X3_test.T, X_est_30.T)])
hr_30 = np.mean(hr_list)

print(f"TEST3: MSE: {mse_30}, HR1: mean {hr_30}")

torch.save(net_mixed, "new_epoch_120_lr_1e_4.pth")

""" for analysis purposes

model = torch.load("/content/drive/My Drive/final models for noise adaptive LISTA--6 march/broad SNR/DDTDA/new_epoch_120_lr_1e_4.pth", weights_only = False)
model.eval()
intermediate_outputs = []

def hook_fn(module, input, output):
    intermediate_outputs.append(output.detach())

hook = model.fc4.register_forward_hook(hook_fn)
X_est_5 = LISTA_test(model, Y1_test, A, device, sigma1_test)

all_outputs = torch.cat(intermediate_outputs, dim=0)
abs_outputs = all_outputs.abs()

mean_abs = abs_outputs.mean()
std_abs = abs_outputs.std()

print("Mean of absolute values:", mean_abs.item())
print("Standard deviation of absolute values:", std_abs.item())


hook.remove()

intermediate_outputs = []

def hook_fn(module, input, output):
    intermediate_outputs.append(output.detach())

hook = model.fc4.register_forward_hook(hook_fn)
X_est_15 = LISTA_test(model, Y2_test, A, device, sigma2_test)

all_outputs = torch.cat(intermediate_outputs, dim=0)
abs_outputs = all_outputs.abs()

mean_abs = abs_outputs.mean()
std_abs = abs_outputs.std()

print("Mean of absolute values:", mean_abs.item())
print("Standard deviation of absolute values:", std_abs.item())


hook.remove()

intermediate_outputs = []

def hook_fn(module, input, output):
    intermediate_outputs.append(output.detach())

hook = model.fc4.register_forward_hook(hook_fn)
X_est_30 = LISTA_test(model, Y3_test, A, device, sigma3_test)

all_outputs = torch.cat(intermediate_outputs, dim=0)
abs_outputs = all_outputs.abs()

mean_abs = abs_outputs.mean()
std_abs = abs_outputs.std()

print("Mean of absolute values:", mean_abs.item())
print("Standard deviation of absolute values:", std_abs.item())


hook.remove()
"""
